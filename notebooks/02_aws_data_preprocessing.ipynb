{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all files in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = s3.Bucket('legalthings-datalake')\n",
    "\n",
    "for file in my_bucket.objects.all():\n",
    "    if file.key.startswith('mongo/'):\n",
    "        print(file.key, file.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files directly from bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_sbi_df = pd.read_csv(smart_open.open('s3://legalthings-datalake/mongo/company_sbi.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_sbi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_json(smart_open.open('s3://legalthings-datalake/mongo/users.json'), encoding=\"utf8\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with one liner json files\n",
    "* ValueError: Unexpected character found when decoding array value (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# licenses_df = pd.read_json(smart_open.open('s3://legalthings-datalake/mongo/licenses.json'), encoding=\"utf8\", lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential fix: reformat json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_model.storage import make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readable_json(\n",
    "    unprocessed_filename: str ='../data/mongo/licenses.json',\n",
    "    preprocessed_dir: str ='../data/preprocess/licenses/',\n",
    "    split_variabele: str = '{\"_id\":',\n",
    "    chunk_size = 4096\n",
    "):\n",
    "    \"\"\"Reformat unreadable oneliner JSON to readable format.\"\"\"\n",
    "    make_folder(preprocessed_dir)\n",
    "    processed_filename = os.path.join(preprocessed_dir,unprocessed_filename.split('/')[-1])\n",
    "    \n",
    "    with open(unprocessed_filename) as f_read:\n",
    "        with open(processed_filename, 'w') as f_write:\n",
    "            for chunk in each_chunk(f_read, chunk_size, split_variabele):\n",
    "                format_json = split_variabele + chunk\n",
    "                if format_json != split_variabele and len(format_json) > 0:\n",
    "                    try:\n",
    "                        reformated_json = json.loads(format_json)            \n",
    "                        f_write.write('{}\\n'.format(json.dumps(reformated_json)))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print('Fail to parse')\n",
    "                        pass\n",
    "\n",
    "def each_chunk(stream, chunk_size, separator):\n",
    "    \"\"\"Separates the one line into separate readable lines.\"\"\"\n",
    "    buffer = ''\n",
    "    while True:  # until EOF\n",
    "        chunk = stream.read(chunk_size)  # I propose 4096 or so\n",
    "        if not chunk:  # EOF?\n",
    "            yield buffer\n",
    "            break\n",
    "            \n",
    "        buffer += chunk\n",
    "        while True:  # until no separator is found\n",
    "            try:\n",
    "                part, buffer = buffer.split(separator, 1)\n",
    "            except ValueError:\n",
    "                break\n",
    "            else:\n",
    "                yield part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed: licenses.json --> parse OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_readable_json(\n",
    "    unprocessed_filename ='../data/mongo/licenses.json',\n",
    "    preprocessed_dir ='../data/preprocess/licenses/',\n",
    "    split_variabele = '{\"_id\":'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_df = pd.read_json(smart_open.open('../data/preprocess/licenses/licenses.json'), encoding=\"utf8\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biggest file: incorporation-processes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Parse into readable JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_readable_json(\n",
    "    unprocessed_filename ='../data/mongo/incorporation-processes.json',\n",
    "    preprocessed_dir ='../data/preprocess/incorporation_processes/',\n",
    "    split_variabele = '{\"_id\":'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of lines in file --> not possible to load in memory\n",
    "* !wc -l ../data/preprocess/incorporation_processes/incorporation-processes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/preprocess/incorporation_processes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for line in open(os.path.join(base_path, 'incorporation-processes.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 ../data/preprocess/incorporation_processes/incorporation-processes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Make separate small files from it that fits in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, n):\n",
    "    \"chunks(ABCDE,2) => AB CD E\"\n",
    "    iterable = iter(iterable)\n",
    "    while True:\n",
    "        # store one line in memory,\n",
    "        # chain it to an iterator on the rest of the chunk\n",
    "        try:\n",
    "            yield chain([next(iterable)], islice(iterable, n-1))\n",
    "        except StopIteration:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_into_multiple_files(\n",
    "    directory = '../data/preprocess/incorporation_processes',\n",
    "    file_to_split = 'incorporation-processes.json',\n",
    "    new_sub_file_name = 'processes',\n",
    "    n_files = 35,\n",
    "    file_type = 'json'\n",
    "):\n",
    "    \"Splits big file into separate files.\"\n",
    "    \n",
    "    file = os.path.join(directory, file_to_split)\n",
    "    num_lines = sum(1 for line in open(file))\n",
    "    l = round(num_lines / n_files)\n",
    "\n",
    "    with open(file) as bigfile:\n",
    "        for i, lines in enumerate(chunks(bigfile, l)):\n",
    "            file_split = os.path.join(directory, 'processes_{}.{}'.format(i, file_type))\n",
    "\n",
    "            with open(file_split, 'w') as f:\n",
    "                f.writelines(lines) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = sum(1 for line in open('../data/preprocess/incorporation_processes/incorporation-processes.json'))\n",
    "n_files = 35\n",
    "l = round(num_lines / n_files) # lines per split file\n",
    "file_large = '../data/preprocess/incorporation_processes/incorporation-processes.json'\n",
    "with open(file_large) as bigfile:\n",
    "    for i, lines in enumerate(chunks(bigfile, l)):\n",
    "        file_split = '../data/preprocess/incorporation_processes/processes_{}.json'.format(i)\n",
    "        with open(file_split, 'w') as f:\n",
    "            f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('../data/preprocess/incorporation_processes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check total number of lines sum up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data/preprocess/incorporation_processes'\n",
    "\n",
    "line_count_orginal = sum(1 for line in open(os.path.join(base_path, 'incorporation-processes.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = 0\n",
    "\n",
    "for i in range(0, 35):\n",
    "    file_name = os.path.join(base_path, 'processes_{}.json'.format(i))\n",
    "    line_count += sum(1 for line in open(file_name))\n",
    "    \n",
    "assert(line_count_orginal == line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subfile of incorporation_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorporation_processes_df = pd.read_json(\n",
    "    smart_open.open('../data/preprocess/incorporation_processes/processes_34.json'), \n",
    "    encoding=\"utf8\", lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupt: emails.json --> parse not well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_readable_json(\n",
    "    unprocessed_filename ='../data/mongo/emails.json',\n",
    "    preprocessed_dir ='../data/preprocess/email/',\n",
    "    split_variabele = '{\"_id\":'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df = pd.read_json(smart_open.open('../data/preprocess/email/emails.json'), encoding=\"utf8\", lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at json chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mongo/licenses.json') as myFile:\n",
    "    for chunk in each_chunk(myFile, chunk_size=4000, separator='{\"_id\":'):\n",
    "        format_json = '{\"_id\": ' + chunk\n",
    "        if format_json != '{\"_id\": ':\n",
    "#             print(format_json)  # not holding in memory, but printing chunk by chunk\n",
    "            print(json.loads(format_json))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
